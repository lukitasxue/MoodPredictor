{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b757c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mood_data.csv\")\n",
    "feature_cols = [\n",
    "    'sleep_hours', 'stress_level', 'exercise_minutes',\n",
    "    'nutrition_quality', 'social_minutes', 'water_liters', 'caffeine_cups'\n",
    "]\n",
    "target_col = 'mood_score'\n",
    "\n",
    "X = df[feature_cols].values   # shape (n_samples, n_features)\n",
    "y = df[target_col].values     # shape (n_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f058b1",
   "metadata": {},
   "source": [
    "y = m * x + b\n",
    "\n",
    "to \n",
    "\n",
    "y = w₁·x₁ + w₂·x₂ + ... + wₙ·xₙ + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6afbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultVarLinearRegressionModel:\n",
    "    def __init__(self):\n",
    "        self.weights = None # holds weights w1,w2,w3,wn\n",
    "        self.bias = 0       # Intercept term (b)\n",
    "\n",
    "\n",
    "    # This uses matrix math (called the Normal Equation) to find the best weights.\n",
    "    def fit(self, X, y): \n",
    "        \"\"\"\n",
    "        Fit the model using the Normal Equation:\n",
    "        w = (X^T X)^-1 X^T y\n",
    "        \"\"\"\n",
    "        ones = np.ones((X.shape[0], 1))        # Column of 1s to simulate bias term\n",
    "        X_b = np.hstack([ones, X])             # Augmented X with bias term\n",
    "        X_transpose = X_b.T\n",
    "\n",
    "        self.theta = np.linalg.inv(X_transpose @ X_b) @ X_transpose @ y\n",
    "\n",
    "        self.bias = self.theta[0]              # First value = intercept\n",
    "        self.weights = self.theta[1:]          # Rest = weights for each feature\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using:\n",
    "        y_pred = X * w + b\n",
    "        \"\"\"\n",
    "        return X @ self.weights + self.bias \n",
    "        # No longer loop manually, instead, this uses NumPy to predict in bulk using matrix multiplication.\n",
    "\n",
    "\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb697b6",
   "metadata": {},
   "source": [
    "### Bias and Ones Matrix (for Matrix Regression)\n",
    "\n",
    "**bias** = shifts the entire prediction up or down.\n",
    "\n",
    "We want to create a ones matrix.\n",
    "\n",
    "Here, I'm simulating 2 sample rows with 7 features.\n",
    "\n",
    "In this case, `X_test.shape[0]` creates 1 bias for each row,  \n",
    "generating a matrix with 2 rows and 1 column of ones.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why do this?\n",
    "\n",
    "Because the bias/intercept term **b** in a regression model isn't multiplied by a feature it's just added at the end:\n",
    "\n",
    "y = w1x1 + w1x2 + ... + 2nxn + b\n",
    "\n",
    "To represent this using matrix multiplication, we trick the math by turning **b** into a weight of 1s:\n",
    "\n",
    "y = θ0*1 + θ1x1 + ... + θnxn\n",
    "\n",
    "So basically:\n",
    "\n",
    "so basically θ0 = b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e3a32db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([[7, 3, 30, 8, 60, 2, 1],\n",
    "                   [6, 5, 20, 7, 50, 1.5, 2]])\n",
    "y_test = np.array([8, 7])\n",
    "one_m_test = np.ones((X_test.shape[0], 1)) # shape 2,1\n",
    "print(f\"{one_m_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c737c",
   "metadata": {},
   "source": [
    "Combine ones array and then transpose it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcf173c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   7.   3.  30.   8.  60.   2.   1. ]\n",
      " [ 1.   6.   5.  20.   7.  50.   1.5  2. ]]\n",
      "(2, 8)\n"
     ]
    }
   ],
   "source": [
    "X_b_test = np.hstack([one_m_test, X_test])    \n",
    "\n",
    "print(f\"{X_b_test}\")\n",
    "print(f\"{X_b_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a863f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   1. ]\n",
      " [ 7.   6. ]\n",
      " [ 3.   5. ]\n",
      " [30.  20. ]\n",
      " [ 8.   7. ]\n",
      " [60.  50. ]\n",
      " [ 2.   1.5]\n",
      " [ 1.   2. ]]\n",
      "\n",
      "(8, 2)\n"
     ]
    }
   ],
   "source": [
    "X_b_transposed = X_b_test.T\n",
    "print(f\"{X_b_transposed}\")\n",
    "print(f\"\\n{X_b_transposed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1043be",
   "metadata": {},
   "source": [
    "@ is for matrix multiplication\n",
    "\n",
    "its the same as np.dot() but more cleaner for this case\n",
    "\n",
    "nwo that we have everything prepared, then we can do normal equation \n",
    "\n",
    "Xt * X = Xt @ X\n",
    "\n",
    "(Xt @ X)^-1 = np.linalg.inv(Xt @ X)\n",
    "\n",
    "only square matrices can be inverted\n",
    "\n",
    "then we multiply all of this with y which are our outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c4bcf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test = (X_b_transposed @ X_b_test) @ X_b_transposed @ y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8195a651",
   "metadata": {},
   "source": [
    "resulting in theta, holding all the learned values, [bias, w1, ..., wn]\n",
    "\n",
    "These values control:\n",
    "\n",
    "Example if the mode learns w_sleep = 0.6 and w_stress = - 0.8 then:\n",
    "- Increasing sleep by 1 hour increases mood by 0.6\n",
    "- increasing stress by 1 unit decreases mood by 0.8\n",
    "\n",
    "so mood = b + w1x1 + w2x2 + ... + wnwn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c07a2b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias: 113929.75\n",
      "Weights: [ 746635.5    443534.75  2909165.     860565.25  6327057.5    202423.125\n",
      "  164802.5  ]\n"
     ]
    }
   ],
   "source": [
    "bias_test = theta_test[0]  # First value = intercept\n",
    "weights_test = theta_test[1:]  # Rest = weights for each feature\n",
    "print(f\"Bias: {bias_test}\")\n",
    "print(f\"Weights: {weights_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2e977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
